{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "C-ROOT_rudy_et_al.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnauldnzegha/DEEP2PDEs/blob/main/C_ROOT_rudy_et_al.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pp5yuDygHZkQ"
      },
      "source": [
        "# PDE-FIND\n",
        "Samuel Rudy, 2016\n",
        "\n",
        "Application on the CROOT PDE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfbuJfIgHZkS",
        "outputId": "33e50340-dc88-45e1-e297-6735730a2a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%pylab inline\n",
        "pylab.rcParams['figure.figsize'] = (12, 8)\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import scipy.io as sio\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wLO0GDCfAwe"
      },
      "source": [
        "## PDE-FIND"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3jymd2PJOaL"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import scipy.sparse as sparse\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse import dia_matrix\n",
        "import itertools\n",
        "import operator\n",
        "\n",
        "##################################################################################\n",
        "##################################################################################\n",
        "#\n",
        "# Functions for taking derivatives.\n",
        "# When in doubt / nice data ===> finite differences\n",
        "#               \\ noisy data ===> polynomials\n",
        "#             \n",
        "##################################################################################\n",
        "##################################################################################\n",
        "\n",
        "def TikhonovDiff(f, dx, lam, d = 1):\n",
        "    \"\"\"\n",
        "    Tikhonov differentiation.\n",
        "    return argmin_g \\|Ag-f\\|_2^2 + lam*\\|Dg\\|_2^2\n",
        "    where A is trapezoidal integration and D is finite differences for first dervative\n",
        "    It looks like it will work well and does for the ODE case but \n",
        "    tends to introduce too much bias to work well for PDEs.  If the data is noisy, try using\n",
        "    polynomials instead.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a few things    \n",
        "    n = len(f)\n",
        "    f = np.matrix(f - f[0]).reshape((n,1))\n",
        "\n",
        "    # Get a trapezoidal approximation to an integral\n",
        "    A = np.zeros((n,n))\n",
        "    for i in range(1, n):\n",
        "        A[i,i] = dx/2\n",
        "        A[i,0] = dx/2\n",
        "        for j in range(1,i): A[i,j] = dx\n",
        "    \n",
        "    e = np.ones(n-1)\n",
        "    D = sparse.diags([e, -e], [1, 0], shape=(n-1, n)).todense() / dx\n",
        "    \n",
        "    # Invert to find derivative\n",
        "    g = np.squeeze(np.asarray(np.linalg.lstsq(A.T.dot(A) + lam*D.T.dot(D),A.T.dot(f))[0]))\n",
        "    \n",
        "    if d == 1: return g\n",
        "\n",
        "    # If looking for a higher order derivative, this one should be smooth so now we can use finite differences\n",
        "    else: return FiniteDiff(g, dx, d-1)\n",
        "    \n",
        "def FiniteDiff(u, dx, d):\n",
        "    \"\"\"\n",
        "    Takes dth derivative data using 2nd order finite difference method (up to d=3)\n",
        "    Works but with poor accuracy for d > 3\n",
        "    \n",
        "    Input:\n",
        "    u = data to be differentiated\n",
        "    dx = Grid spacing.  Assumes uniform spacing\n",
        "    \"\"\"\n",
        "    \n",
        "    n = u.size\n",
        "    ux = np.zeros(n, dtype=np.complex64)\n",
        "    \n",
        "    if d == 1:\n",
        "        for i in range(1,n-1):\n",
        "            ux[i] = (u[i+1]-u[i-1]) / (2*dx)\n",
        "        \n",
        "        ux[0] = (-3.0/2*u[0] + 2*u[1] - u[2]/2) / dx\n",
        "        ux[n-1] = (3.0/2*u[n-1] - 2*u[n-2] + u[n-3]/2) / dx\n",
        "        return ux\n",
        "    \n",
        "    if d == 2:\n",
        "        for i in range(1,n-1):\n",
        "            ux[i] = (u[i+1]-2*u[i]+u[i-1]) / dx**2\n",
        "        \n",
        "        ux[0] = (2*u[0] - 5*u[1] + 4*u[2] - u[3]) / dx**2\n",
        "        ux[n-1] = (2*u[n-1] - 5*u[n-2] + 4*u[n-3] - u[n-4]) / dx**2\n",
        "        return ux\n",
        "    \n",
        "    if d == 3:\n",
        "        for i in range(2,n-2):\n",
        "            ux[i] = (u[i+2]/2-u[i+1]+u[i-1]-u[i-2]/2) / dx**3\n",
        "        \n",
        "        ux[0] = (-2.5*u[0]+9*u[1]-12*u[2]+7*u[3]-1.5*u[4]) / dx**3\n",
        "        ux[1] = (-2.5*u[1]+9*u[2]-12*u[3]+7*u[4]-1.5*u[5]) / dx**3\n",
        "        ux[n-1] = (2.5*u[n-1]-9*u[n-2]+12*u[n-3]-7*u[n-4]+1.5*u[n-5]) / dx**3\n",
        "        ux[n-2] = (2.5*u[n-2]-9*u[n-3]+12*u[n-4]-7*u[n-5]+1.5*u[n-6]) / dx**3\n",
        "        return ux\n",
        "    \n",
        "    if d > 3:\n",
        "        return FiniteDiff(FiniteDiff(u,dx,3), dx, d-3)\n",
        "    \n",
        "def ConvSmoother(x, p, sigma):\n",
        "    \"\"\"\n",
        "    Smoother for noisy data\n",
        "    \n",
        "    Inpute = x, p, sigma\n",
        "    x = one dimensional series to be smoothed\n",
        "    p = width of smoother\n",
        "    sigma = standard deviation of gaussian smoothing kernel\n",
        "    \"\"\"\n",
        "    \n",
        "    n = len(x)\n",
        "    y = np.zeros(n, dtype=np.complex64)\n",
        "    g = np.exp(-np.power(np.linspace(-p,p,2*p),2)/(2.0*sigma**2))\n",
        "\n",
        "    for i in range(n):\n",
        "        a = max([i-p,0])\n",
        "        b = min([i+p,n])\n",
        "        c = max([0, p-i])\n",
        "        d = min([2*p,p+n-i])\n",
        "        y[i] = np.sum(np.multiply(x[a:b], g[c:d]))/np.sum(g[c:d])\n",
        "        \n",
        "    return y\n",
        "\n",
        "def PolyDiff(u, x, deg = 3, diff = 1, width = 5):\n",
        "    \n",
        "    \"\"\"\n",
        "    u = values of some function\n",
        "    x = x-coordinates where values are known\n",
        "    deg = degree of polynomial to use\n",
        "    diff = maximum order derivative we want\n",
        "    width = width of window to fit to polynomial\n",
        "    This throws out the data close to the edges since the polynomial derivative only works\n",
        "    well when we're looking at the middle of the points fit.\n",
        "    \"\"\"\n",
        "\n",
        "    u = u.flatten()\n",
        "    x = x.flatten()\n",
        "\n",
        "    n = len(x)\n",
        "    du = np.zeros((n - 2*width,diff))\n",
        "\n",
        "    # Take the derivatives in the center of the domain\n",
        "    for j in range(width, n-width):\n",
        "\n",
        "        points = np.arange(j - width, j + width)\n",
        "\n",
        "        # Fit to a Chebyshev polynomial\n",
        "        # this is the same as any polynomial since we're on a fixed grid but it's better conditioned :)\n",
        "        poly = np.polynomial.chebyshev.Chebyshev.fit(x[points],u[points],deg)\n",
        "\n",
        "        # Take derivatives\n",
        "        for d in range(1,diff+1):\n",
        "            du[j-width, d-1] = poly.deriv(m=d)(x[j])\n",
        "\n",
        "    return du\n",
        "\n",
        "def PolyDiffPoint(u, x, deg = 3, diff = 1, index = None):\n",
        "    \n",
        "    \"\"\"\n",
        "    Same as above but now just looking at a single point\n",
        "    u = values of some function\n",
        "    x = x-coordinates where values are known\n",
        "    deg = degree of polynomial to use\n",
        "    diff = maximum order derivative we want\n",
        "    \"\"\"\n",
        "    \n",
        "    n = len(x)\n",
        "    if index == None: index = (n-1)/2\n",
        "\n",
        "    # Fit to a Chebyshev polynomial\n",
        "    # better conditioned than normal polynomials\n",
        "    poly = np.polynomial.chebyshev.Chebyshev.fit(x,u,deg)\n",
        "    \n",
        "    # Take derivatives\n",
        "    derivatives = []\n",
        "    for d in range(1,diff+1):\n",
        "        derivatives.append(poly.deriv(m=d)(x[index]))\n",
        "        \n",
        "    return derivatives\n",
        "\n",
        "##################################################################################\n",
        "##################################################################################\n",
        "#\n",
        "# Functions specific to PDE-FIND\n",
        "#               \n",
        "##################################################################################\n",
        "##################################################################################\n",
        "\n",
        "def build_Theta(data, derivatives, derivatives_description, P, data_description = None):\n",
        "    n,d = data.shape\n",
        "    m, d2 = derivatives.shape\n",
        "    if n != m: raise Exception('dimension error')\n",
        "    if data_description is not None: \n",
        "        if len(data_description) != d: raise Exception('data descrption error')\n",
        "    \n",
        "    # Create a list of all polynomials in d variables up to degree P\n",
        "    rhs_functions = {}\n",
        "    f = lambda x, y : np.prod(np.power(list(x), list(y)))\n",
        "    powers = []            \n",
        "    for p in range(1,P+1):\n",
        "            size = d + p - 1\n",
        "            for indices in itertools.combinations(range(size), d-1):\n",
        "                starts = [0] + [index+1 for index in indices]\n",
        "                stops = indices + (size,)\n",
        "                powers.append(tuple(map(operator.sub, stops, starts)))\n",
        "    for power in powers: rhs_functions[power] = [lambda x, y = power: f(x,y), power]\n",
        "\n",
        "    # First column of Theta is just ones.\n",
        "    Theta = np.ones((n,1), dtype=np.complex64)\n",
        "    descr = ['']\n",
        "    \n",
        "    # Add the derivaitves onto Theta\n",
        "    for D in range(1,derivatives.shape[1]):\n",
        "        Theta = np.hstack([Theta, derivatives[:,D].reshape(n,1)])\n",
        "        descr.append(derivatives_description[D])\n",
        "        \n",
        "    # Add on derivatives times polynomials\n",
        "    for D in range(derivatives.shape[1]):\n",
        "        for k in rhs_functions.keys():\n",
        "            func = rhs_functions[k][0]\n",
        "            new_column = np.zeros((n,1), dtype=np.complex64)\n",
        "            for i in range(n):\n",
        "                new_column[i] = func(data[i,:])*derivatives[i,D]\n",
        "            Theta = np.hstack([Theta, new_column])\n",
        "            if data_description is None: descr.append(str(rhs_functions[k][1]) + derivatives_description[D])\n",
        "            else:\n",
        "                function_description = ''\n",
        "                for j in range(d):\n",
        "                    if rhs_functions[k][1][j] != 0:\n",
        "                        if rhs_functions[k][1][j] == 1:\n",
        "                            function_description = function_description + data_description[j]\n",
        "                        else:\n",
        "                            function_description = function_description + data_description[j] + '^' + str(rhs_functions[k][1][j])\n",
        "                descr.append(function_description + derivatives_description[D])\n",
        "\n",
        "    return Theta, descr\n",
        "\n",
        "def build_linear_system(u, dt, dx, D = 3, P = 3,time_diff = 'poly',space_diff = 'poly',lam_t = None,lam_x = None, width_x = None,width_t = None, deg_x = 5,deg_t = None,sigma = 2):\n",
        "    \"\"\"\n",
        "    Constructs a large linear system to use in later regression for finding PDE.  \n",
        "    This function works when we are not subsampling the data or adding in any forcing.\n",
        "    Input:\n",
        "        Required:\n",
        "            u = data to be fit to a pde\n",
        "            dt = temporal grid spacing\n",
        "            dx = spatial grid spacing\n",
        "        Optional:\n",
        "            D = max derivative to include in rhs (default = 3)\n",
        "            P = max power of u to include in rhs (default = 3)\n",
        "            time_diff = method for taking time derivative\n",
        "                        options = 'poly', 'FD', 'FDconv','TV'\n",
        "                        'poly' (default) = interpolation with polynomial \n",
        "                        'FD' = standard finite differences\n",
        "                        'FDconv' = finite differences with convolutional smoothing \n",
        "                                   before and after along x-axis at each timestep\n",
        "                        'Tik' = Tikhonov (takes very long time)\n",
        "            space_diff = same as time_diff with added option, 'Fourier' = differentiation via FFT\n",
        "            lam_t = penalization for L2 norm of second time derivative\n",
        "                    only applies if time_diff = 'TV'\n",
        "                    default = 1.0/(number of timesteps)\n",
        "            lam_x = penalization for L2 norm of (n+1)st spatial derivative\n",
        "                    default = 1.0/(number of gridpoints)\n",
        "            width_x = number of points to use in polynomial interpolation for x derivatives\n",
        "                      or width of convolutional smoother in x direction if using FDconv\n",
        "            width_t = number of points to use in polynomial interpolation for t derivatives\n",
        "            deg_x = degree of polynomial to differentiate x\n",
        "            deg_t = degree of polynomial to differentiate t\n",
        "            sigma = standard deviation of gaussian smoother\n",
        "                    only applies if time_diff = 'FDconv'\n",
        "                    default = 2\n",
        "    Output:\n",
        "        ut = column vector of length u.size\n",
        "        R = matrix with ((D+1)*(P+1)) of column, each as large as ut\n",
        "        rhs_description = description of what each column in R is\n",
        "    \"\"\"\n",
        "\n",
        "    n, m = u.shape\n",
        "\n",
        "    if width_x == None: width_x = n/10\n",
        "    if width_t == None: width_t = m/10\n",
        "    if deg_t == None: deg_t = deg_x\n",
        "\n",
        "    # If we're using polynomials to take derviatives, then we toss the data around the edges.\n",
        "    if time_diff == 'poly': \n",
        "        m2 = m-2*width_t\n",
        "        offset_t = width_t\n",
        "    else: \n",
        "        m2 = m\n",
        "        offset_t = 0\n",
        "    if space_diff == 'poly': \n",
        "        n2 = n-2*width_x\n",
        "        offset_x = width_x\n",
        "    else: \n",
        "        n2 = n\n",
        "        offset_x = 0\n",
        "\n",
        "    if lam_t == None: lam_t = 1.0/m\n",
        "    if lam_x == None: lam_x = 1.0/n\n",
        "\n",
        "    ########################\n",
        "    # First take the time derivaitve for the left hand side of the equation\n",
        "    ########################\n",
        "    ut = np.zeros((n2,m2), dtype=np.complex64)\n",
        "\n",
        "    if time_diff == 'FDconv':\n",
        "        Usmooth = np.zeros((n,m), dtype=np.complex64)\n",
        "        # Smooth across x cross-sections\n",
        "        for j in range(m):\n",
        "            Usmooth[:,j] = ConvSmoother(u[:,j],width_t,sigma)\n",
        "        # Now take finite differences\n",
        "        for i in range(n2):\n",
        "            ut[i,:] = FiniteDiff(Usmooth[i + offset_x,:],dt,1)\n",
        "\n",
        "    elif time_diff == 'poly':\n",
        "        T= np.linspace(0,(m-1)*dt,m)\n",
        "        for i in range(n2):\n",
        "            ut[i,:] = PolyDiff(u[i+offset_x,:],T,diff=1,width=width_t,deg=deg_t)[:,0]\n",
        "\n",
        "    elif time_diff == 'Tik':\n",
        "        for i in range(n2):\n",
        "            ut[i,:] = TikhonovDiff(u[i + offset_x,:], dt, lam_t)\n",
        "\n",
        "    else:\n",
        "        for i in range(n2):\n",
        "            ut[i,:] = FiniteDiff(u[i + offset_x,:],dt,1)\n",
        "    \n",
        "    ut = np.reshape(ut, (n2*m2,1), order='F')\n",
        "\n",
        "    ########################\n",
        "    # Now form the rhs one column at a time, and record what each one is\n",
        "    ########################\n",
        "\n",
        "    u2 = u[offset_x:n-offset_x,offset_t:m-offset_t]\n",
        "    Theta = np.zeros((n2*m2, (D+1)*(P+1)), dtype=np.complex64)\n",
        "    ux = np.zeros((n2,m2), dtype=np.complex64)\n",
        "    rhs_description = ['' for i in range((D+1)*(P+1))]\n",
        "\n",
        "    if space_diff == 'poly': \n",
        "        Du = {}\n",
        "        for i in range(m2):\n",
        "            Du[i] = PolyDiff(u[:,i+offset_t],np.linspace(0,(n-1)*dx,n),diff=D,width=width_x,deg=deg_x)\n",
        "    if space_diff == 'Fourier': ik = 1j*np.fft.fftfreq(n)*n\n",
        "        \n",
        "    for d in range(D+1):\n",
        "\n",
        "        if d > 0:\n",
        "            for i in range(m2):\n",
        "                if space_diff == 'Tik': ux[:,i] = TikhonovDiff(u[:,i+offset_t], dx, lam_x, d=d)\n",
        "                elif space_diff == 'FDconv':\n",
        "                    Usmooth = ConvSmoother(u[:,i+offset_t],width_x,sigma)\n",
        "                    ux[:,i] = FiniteDiff(Usmooth,dx,d)\n",
        "                elif space_diff == 'FD': ux[:,i] = FiniteDiff(u[:,i+offset_t],dx,d)\n",
        "                elif space_diff == 'poly': ux[:,i] = Du[i][:,d-1]\n",
        "                elif space_diff == 'Fourier': ux[:,i] = np.fft.ifft(ik**d*np.fft.fft(ux[:,i]))\n",
        "        else: ux = np.zeros((n2,m2), dtype=np.complex64) \n",
        "            \n",
        "        for p in range(P+1):\n",
        "            Theta[:, d*(P+1)+p] = np.reshape(np.multiply(ux, np.power(u2,p)), (n2*m2), order='F')\n",
        "\n",
        "            if p == 1: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+'u'\n",
        "            elif p>1: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+'u^' + str(p)\n",
        "            if d > 0: rhs_description[d*(P+1)+p] = rhs_description[d*(P+1)+p]+\\\n",
        "                                                   'u_{' + ''.join(['x' for _ in range(d)]) + '}'\n",
        "\n",
        "    return ut, Theta, rhs_description\n",
        "\n",
        "def print_pde(w, rhs_description, ut = 'u_t'):\n",
        "    pde = ut + ' = '\n",
        "    first = True\n",
        "    for i in range(len(w)):\n",
        "        if w[i] != 0:\n",
        "            if not first:\n",
        "                pde = pde + ' + '\n",
        "            pde = pde + \"(%05f %+05fi)\" % (w[i].real, w[i].imag) + rhs_description[i] + \"\\n   \"\n",
        "            first = False\n",
        "    print (pde)\n",
        "\n",
        "##################################################################################\n",
        "##################################################################################\n",
        "#\n",
        "# Functions for sparse regression.\n",
        "#               \n",
        "##################################################################################\n",
        "##################################################################################\n",
        "\n",
        "def TrainSTRidge(R, Ut, lam, d_tol, maxit = 25, STR_iters = 10, l0_penalty = None, normalize = 2, split = 0.8, print_best_tol = False):\n",
        "    \"\"\"\n",
        "    This function trains a predictor using STRidge.\n",
        "    It runs over different values of tolerance and trains predictors on a training set, then evaluates them \n",
        "    using a loss function on a holdout set.\n",
        "    Please note published article has typo.  Loss function used here for model selection evaluates fidelity using 2-norm,\n",
        "    not squared 2-norm.\n",
        "    \"\"\"\n",
        "\n",
        "    # Split data into 80% training and 20% test, then search for the best tolderance.\n",
        "    np.random.seed(0) # for consistancy\n",
        "    n,_ = R.shape\n",
        "    train = np.random.choice(n, int(n*split), replace = False)\n",
        "    test = [i for i in np.arange(n) if i not in train]\n",
        "    TrainR = R[train,:]\n",
        "    TestR = R[test,:]\n",
        "    TrainY = Ut[train,:]\n",
        "    TestY = Ut[test,:]\n",
        "    D = TrainR.shape[1]       \n",
        "\n",
        "    # Set up the initial tolerance and l0 penalty\n",
        "    d_tol = float(d_tol)\n",
        "    tol = d_tol\n",
        "    if l0_penalty == None: l0_penalty = 0.001*np.linalg.cond(R)\n",
        "\n",
        "    # Get the standard least squares estimator\n",
        "    w = np.zeros((D,1))\n",
        "    w_best = np.linalg.lstsq(TrainR, TrainY)[0]\n",
        "    err_best = np.linalg.norm(TestY - TestR.dot(w_best), 2) + l0_penalty*np.count_nonzero(w_best)\n",
        "    tol_best = 0\n",
        "\n",
        "    # Now increase tolerance until test performance decreases\n",
        "    for iter in range(maxit):\n",
        "\n",
        "        # Get a set of coefficients and error\n",
        "        w = STRidge(R,Ut,lam,STR_iters,tol,normalize = normalize)\n",
        "        err = np.linalg.norm(TestY - TestR.dot(w), 2) + l0_penalty*np.count_nonzero(w)\n",
        "\n",
        "        # Has the accuracy improved?\n",
        "        if err <= err_best:\n",
        "            err_best = err\n",
        "            w_best = w\n",
        "            tol_best = tol\n",
        "            tol = tol + d_tol\n",
        "\n",
        "        else:\n",
        "            tol = max([0,tol - 2*d_tol])\n",
        "            d_tol  = 2*d_tol / (maxit - iter)\n",
        "            tol = tol + d_tol\n",
        "\n",
        "    if print_best_tol: print (\"Optimal tolerance:\", tol_best)\n",
        "\n",
        "    return w_best\n",
        "\n",
        "def Lasso(X0, Y, lam, w = np.array([0]), maxit = 100, normalize = 2):\n",
        "    \"\"\"\n",
        "    Uses accelerated proximal gradient (FISTA) to solve Lasso\n",
        "    argmin (1/2)*||Xw-Y||_2^2 + lam||w||_1\n",
        "    \"\"\"\n",
        "    \n",
        "    # Obtain size of X\n",
        "    n,d = X0.shape\n",
        "    X = np.zeros((n,d), dtype=np.complex64)\n",
        "    Y = Y.reshape(n,1)\n",
        "    \n",
        "    # Create w if none is given\n",
        "    if w.size != d:\n",
        "        w = np.zeros((d,1), dtype=np.complex64)\n",
        "    w_old = np.zeros((d,1), dtype=np.complex64)\n",
        "        \n",
        "    # Initialize a few other parameters\n",
        "    converge = 0\n",
        "    objective = np.zeros((maxit,1))\n",
        "    \n",
        "    # First normalize data\n",
        "    if normalize != 0:\n",
        "        Mreg = np.zeros((d,1))\n",
        "        for i in range(0,d):\n",
        "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
        "            X[:,i] = Mreg[i]*X0[:,i]\n",
        "    else: X = X0\n",
        "\n",
        "    # Lipschitz constant of gradient of smooth part of loss function\n",
        "    L = np.linalg.norm(X.T.dot(X),2)\n",
        "    \n",
        "    # Now loop until converged or max iterations\n",
        "    for iters in range(0, maxit):\n",
        "         \n",
        "        # Update w\n",
        "        z = w + iters/float(iters+1)*(w - w_old)\n",
        "        w_old = w\n",
        "        z = z - X.T.dot(X.dot(z)-Y)/L\n",
        "        for j in range(d): w[j] = np.multiply(np.sign(z[j]), np.max([abs(z[j])-lam/L,0]))\n",
        "\n",
        "        # Could put in some sort of break condition based on convergence here.\n",
        "    \n",
        "    # Now that we have the sparsity pattern, used least squares.\n",
        "    biginds = np.where(w != 0)[0]\n",
        "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],Y)[0]\n",
        "\n",
        "    # Finally, reverse the regularization so as to be able to use with raw data\n",
        "    if normalize != 0: return np.multiply(Mreg,w)\n",
        "    else: return w\n",
        "\n",
        "def ElasticNet(X0, Y, lam1, lam2, w = np.array([0]), maxit = 100, normalize = 2):\n",
        "    \"\"\"\n",
        "    Uses accelerated proximal gradient (FISTA) to solve elastic net\n",
        "    argmin (1/2)*||Xw-Y||_2^2 + lam_1||w||_1 + (1/2)*lam_2||w||_2^2\n",
        "    \"\"\"\n",
        "    \n",
        "    # Obtain size of X\n",
        "    n,d = X0.shape\n",
        "    X = np.zeros((n,d), dtype=np.complex64)\n",
        "    Y = Y.reshape(n,1)\n",
        "    \n",
        "    # Create w if none is given\n",
        "    if w.size != d:\n",
        "        w = np.zeros((d,1), dtype=np.complex64)\n",
        "    w_old = np.zeros((d,1), dtype=np.complex64)\n",
        "        \n",
        "    # Initialize a few other parameters\n",
        "    converge = 0\n",
        "    objective = np.zeros((maxit,1))\n",
        "    \n",
        "    # First normalize data\n",
        "    if normalize != 0:\n",
        "        Mreg = np.zeros((d,1))\n",
        "        for i in range(0,d):\n",
        "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
        "            X[:,i] = Mreg[i]*X0[:,i]\n",
        "    else: X = X0\n",
        "\n",
        "    # Lipschitz constant of gradient of smooth part of loss function\n",
        "    L = np.linalg.norm(X.T.dot(X),2) + lam2\n",
        "    \n",
        "    # Now loop until converged or max iterations\n",
        "    for iters in range(0, maxit):\n",
        "         \n",
        "        # Update w\n",
        "        z = w + iters/float(iters+1)*(w - w_old)\n",
        "        w_old = w\n",
        "        z = z - (lam2*z + X.T.dot(X.dot(z)-Y))/L\n",
        "        for j in range(d): w[j] = np.multiply(np.sign(z[j]), np.max([abs(z[j])-lam1/L,0]))\n",
        "\n",
        "        # Could put in some sort of break condition based on convergence here.\n",
        "    \n",
        "    # Now that we have the sparsity pattern, used least squares.\n",
        "    biginds = np.where(w != 0)[0]\n",
        "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],Y)[0]\n",
        "\n",
        "    # Finally, reverse the regularization so as to be able to use with raw data\n",
        "    if normalize != 0: return np.multiply(Mreg,w)\n",
        "    else: return w\n",
        "    \n",
        "def STRidge(X0, y, lam, maxit, tol, normalize = 2, print_results = False):\n",
        "    \"\"\"\n",
        "    Sequential Threshold Ridge Regression algorithm for finding (hopefully) sparse \n",
        "    approximation to X^{-1}y.  The idea is that this may do better with correlated observables.\n",
        "    This assumes y is only one column\n",
        "    \"\"\"\n",
        "\n",
        "    n,d = X0.shape\n",
        "    X = np.zeros((n,d), dtype=np.complex64)\n",
        "    # First normalize data\n",
        "    if normalize != 0:\n",
        "        Mreg = np.zeros((d,1))\n",
        "        for i in range(0,d):\n",
        "            Mreg[i] = 1.0/(np.linalg.norm(X0[:,i],normalize))\n",
        "            X[:,i] = Mreg[i]*X0[:,i]\n",
        "    else: X = X0\n",
        "    \n",
        "    # Get the standard ridge esitmate\n",
        "    if lam != 0: w = np.linalg.lstsq(X.T.dot(X) + lam*np.eye(d),X.T.dot(y))[0]\n",
        "    else: w = np.linalg.lstsq(X,y)[0]\n",
        "    num_relevant = d\n",
        "    biginds = np.where( abs(w) > tol)[0]\n",
        "    \n",
        "    # Threshold and continue\n",
        "    for j in range(maxit):\n",
        "\n",
        "        # Figure out which items to cut out\n",
        "        smallinds = np.where( abs(w) < tol)[0]\n",
        "        new_biginds = [i for i in range(d) if i not in smallinds]\n",
        "            \n",
        "        # If nothing changes then stop\n",
        "        if num_relevant == len(new_biginds): break\n",
        "        else: num_relevant = len(new_biginds)\n",
        "            \n",
        "        # Also make sure we didn't just lose all the coefficients\n",
        "        if len(new_biginds) == 0:\n",
        "            if j == 0: \n",
        "                #if print_results: print \"Tolerance too high - all coefficients set below tolerance\"\n",
        "                return w\n",
        "            else: break\n",
        "        biginds = new_biginds\n",
        "        \n",
        "        # Otherwise get a new guess\n",
        "        w[smallinds] = 0\n",
        "        if lam != 0: w[biginds] = np.linalg.lstsq(X[:, biginds].T.dot(X[:, biginds]) + lam*np.eye(len(biginds)),X[:, biginds].T.dot(y))[0]\n",
        "        else: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
        "\n",
        "    # Now that we have the sparsity pattern, use standard least squares to get w\n",
        "    if biginds != []: w[biginds] = np.linalg.lstsq(X[:, biginds],y)[0]\n",
        "    \n",
        "    if normalize != 0: return np.multiply(Mreg,w)\n",
        "    else: return w\n",
        "    \n",
        "def FoBaGreedy(X, y, epsilon = 0.1, maxit_f = 100, maxit_b = 5, backwards_freq = 5):\n",
        "    \"\"\"\n",
        "    Forward-Backward greedy algorithm for sparse regression.\n",
        "    See Zhang, Tom. 'Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear\n",
        "    Models', NIPS, 2008\n",
        "    \"\"\"\n",
        "\n",
        "    n,d = X.shape\n",
        "    F = {}\n",
        "    F[0] = set()\n",
        "    w = {}\n",
        "    w[0] = np.zeros((d,1))\n",
        "    k = 0\n",
        "    delta = {}\n",
        "\n",
        "    for forward_iter in range(maxit_f):\n",
        "\n",
        "        k = k+1\n",
        "\n",
        "        # forward step\n",
        "        zero_coeffs = np.where(w[k-1] == 0)[0]\n",
        "        err_after_addition = []\n",
        "        residual = y - X.dot(w[k-1])\n",
        "        for i in zero_coeffs:\n",
        "\n",
        "            # Per figure 3 line 8 in paper, do not retrain old variables.\n",
        "            # Only look for optimal alpha, which is solving for new w iff X is unitary\n",
        "            alpha = X[:,i].T.dot(residual)/np.linalg.norm(X[:,i])**2\n",
        "\n",
        "            w_added = np.copy(w[k-1])\n",
        "            w_added[i] = alpha\n",
        "            err_after_addition.append(np.linalg.norm(X.dot(w_added)-y))\n",
        "        i = zero_coeffs[np.argmin(err_after_addition)]\n",
        "        \n",
        "        F[k] = F[k-1].union({i})\n",
        "        w[k] = np.zeros((d,1), dtype=np.complex64)\n",
        "        w[k][list(F[k])] = np.linalg.lstsq(X[:, list(F[k])], y)[0]\n",
        "\n",
        "        # check for break condition\n",
        "        delta[k] = np.linalg.norm(X.dot(w[k-1]) - y) - np.linalg.norm(X.dot(w[k]) - y)\n",
        "        if delta[k] < epsilon: return w[k-1]\n",
        "\n",
        "        # backward step, do once every few forward steps\n",
        "        if forward_iter % backwards_freq == 0 and forward_iter > 0:\n",
        "\n",
        "            for backward_iter in range(maxit_b):\n",
        "\n",
        "                non_zeros = np.where(w[k] != 0)[0]\n",
        "                err_after_simplification = []\n",
        "                for j in non_zeros:\n",
        "                    w_simple = np.copy(w[k])\n",
        "                    w_simple[j] = 0\n",
        "                    err_after_simplification.append(np.linalg.norm(X.dot(w_simple) - y))\n",
        "                j = np.argmin(err_after_simplification)\n",
        "                w_simple = np.copy(w[k])\n",
        "                w_simple[non_zeros[j]] = 0\n",
        "\n",
        "                # check for break condition on backward step\n",
        "                delta_p = err_after_simplification[j] - np.linalg.norm(X.dot(w[k]) - y)\n",
        "                if delta_p > 0.5*delta[k]: break\n",
        "\n",
        "                k = k-1;\n",
        "                F[k] = F[k+1].difference({j})\n",
        "                w[k] = np.zeros((d,1))\n",
        "                w[k][list(F[k])] = np.linalg.lstsq(X[:, list(F[k])], y)[0]\n",
        "\n",
        "    return w[k] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWkalIclfLOQ"
      },
      "source": [
        "## C-ROOT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ_C3JdSHZkY"
      },
      "source": [
        "Load in the data.\n",
        "\n",
        "- $u_t =2u-2u_x - 0.5u_y + 1.5u_{xx}+ 1.5u_{yy} $"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnakhmDliTfb"
      },
      "source": [
        "def derivativeT(t,x,y, v1=2, v2=0.5, p=-2, d=1.5):\n",
        "  c_0=3\n",
        "  a=0.5\n",
        "  x0,y0=0,0\n",
        "  A=a**2/(a**2+t*d)\n",
        "  dA=(-d*a**2)/(a**2+t*d)**2\n",
        "  Kx=(x-x0-t*v1)**2\n",
        "  Ky=(y-y0-t*v2)**2\n",
        "  Kt=4*(a**2 +t*d)\n",
        "  K= np.exp(-(Kx +Ky )/(Kt))\n",
        "  \n",
        "  dKx=-2*v1*(x-x0-t*v1)\n",
        "  dKy=-2*v2*(y-y0-t*v2)\n",
        "  dKt=4*d\n",
        " \n",
        "  dK=-(((dKx+dKy)*Kt - dKt*(Kx+Ky))/Kt**2)*K\n",
        "  E=np.exp(-p*t)\n",
        "  KE=K*E\n",
        "  dE=-p*np.exp(-p*t)\n",
        "  dKE=dK*E + dE*K\n",
        "  \n",
        "  dAKE=dA*KE + dKE*A\n",
        "  return c_0*dAKE\n",
        "  \n",
        "def derivativeX(t,x,y, v1=0.5, v2=0.1, p=-0.1, d=0.1, x0=0,y0=0):\n",
        "  c_0=3\n",
        "  a=0.5\n",
        "  x0,y0=0,0\n",
        "  AE=(a**2/(a**2+t*d))*np.exp(-p*t)\n",
        "  Kx=(x-x0-t*v1)**2\n",
        "  Ky=(y-y0-t*v2)**2\n",
        "  Kt=4*(a**2 +t*d)\n",
        "  K= np.exp(-(Kx +Ky )/(Kt))\n",
        "  \n",
        "  dKx=2*(x-x0-t*v1)\n",
        "  dKy=0\n",
        "  \n",
        "  d2Kx=2\n",
        "  \n",
        "  dK=-((dKx+dKy)/Kt)*K\n",
        "  \n",
        "  dAKE=AE*dK\n",
        "  d2AKE=AE*( -(d2Kx/Kt)*K - dK*(dKx+dKy)/Kt)\n",
        "  \n",
        "  return [c_0*dAKE, c_0*d2AKE]\n",
        "  \n",
        "  \n",
        "  \n",
        "def derivativeY(t,x,y, v1=0.5, v2=0.1,  p=-0.1, d=0.1):\n",
        "  c_0=3\n",
        "  a=0.5\n",
        "  x0,y0=0,0\n",
        "  AE=(a**2/(a**2+t*d))*np.exp(-p*t)\n",
        "  Kx=(x-x0-t*v1)**2\n",
        "  Ky=(y-y0-t*v2)**2\n",
        "  Kt=4*(a**2 +t*d)\n",
        "  K= np.exp(-(Kx +Ky )/(Kt))\n",
        "  \n",
        "  dKx=0\n",
        "  dKy=2*(y-y0-t*v2)\n",
        "  \n",
        "  d2Ky=2\n",
        "  \n",
        "  dK=-((dKx+dKy)/Kt)*K\n",
        "  \n",
        "  dAKE=AE*dK\n",
        "  d2AKE=AE*( -(d2Ky/Kt)*K - dK*(dKx+dKy)/Kt)\n",
        "  \n",
        "  return [c_0*dAKE, c_0*d2AKE]\n",
        "\n",
        "\n",
        "def get_derivative(TXY,v1=2, v2=0.5, p=-2, d=1.5):\n",
        "  dev=[]\n",
        "  for (t,x,y) in TXY:\n",
        "    d=[]\n",
        "    d.append(derivativeT(t,x,y,v1=2, v2=0.5, p=-2, d=1.5))\n",
        "    d.extend(derivativeX(t,x,y,v1=2, v2=0.5, p=-2, d=1.5))\n",
        "    d.extend(derivativeY(t,x,y,v1=2, v2=0.5, p=-2, d=1.5))\n",
        "    dev.append(d)\n",
        "  return np.array(dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lp8H3JmIK-q"
      },
      "source": [
        "def u_anal(t,x,y,p,v1,v2,d):\n",
        "  c_0=3\n",
        "  a=0.5\n",
        "  x0,y0=0,0\n",
        "  C=c_0 * (a**2/(a**2+t*d))\n",
        "  K= np.exp(-((x-x0-t*v1)**2 + (y-y0-t*v2)**2)/(4*(a**2 +t*d)))\n",
        "  u=C*K*np.exp(-p*t)\n",
        "  return u\n",
        "\n",
        "def get_data(nb_t,nb_xy,p,v1,v2,d):\n",
        "    min_t, max_t =0 , 0.99\n",
        "    min_xy, max_xy =-1, 1\n",
        "    t = np.linspace(min_t, max_t, nb_t )\n",
        "    x = np.linspace(min_xy,max_xy, nb_xy )\n",
        "    y = np.linspace(min_xy,max_xy, nb_xy)\n",
        "    x_,y_=np.meshgrid(x, y)\n",
        "    u=[]\n",
        "    for i in range(0,len(t)):\n",
        "      u_i=[u_anal(t[i],x,y,p,v1,v2,d) for (x,y) in list(zip(x_,y_))]\n",
        "      u.append(np.array(u_i))\n",
        "    return t,x,y, np.array(u)\n",
        "  \n",
        "nb_t,nb_xy=250,50\n",
        "n,m,steps=50,50,250\n",
        "p=-2\n",
        "v1,v2=2,0.5\n",
        "d=1.5\n",
        "c_0=3\n",
        "a=0.5\n",
        "dt=0.004\n",
        "dx=0.1\n",
        "dy=0.1\n",
        "t,x,y,Un=get_data(nb_t,nb_xy,p,v1,v2,d) \n",
        "#Un=Un.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAQnSoo3wPv3"
      },
      "source": [
        "def flaten_data(t,x,y,u):\n",
        "  xi,yi=np.meshgrid(x, y)\n",
        "  xi,yi=xi.flatten(),yi.flatten()\n",
        "  x_train=[]\n",
        "  for ti in t:\n",
        "    x_train.append(list(zip([ti]*len(xi),xi,yi)))\n",
        "  x_train=np.array(x_train)\n",
        "  return x_train.reshape(-1,3), u.flatten()\n",
        "\n",
        "x_train,y_train=flaten_data(t,x,y,Un)\n",
        "Utxy_anal=get_derivative(x_train)\n",
        "Ut_a=Utxy_anal[:,0]\n",
        "Ux_a,Uxx_a=Utxy_anal[:,1],Utxy_anal[:,2]\n",
        "Uy_a,Uyy_a=Utxy_anal[:,3],Utxy_anal[:,4]\n",
        "u_f=Un.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "as9JpLR7mo9R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMZuwn6nHZkZ"
      },
      "source": [
        "# Cut out the portion of the data before the cylinder\n",
        "xmin = -1\n",
        "xmax = 1\n",
        "ymin = -1\n",
        "ymax = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed4XaEBlHZkd"
      },
      "source": [
        "Here we take the SVD of the data and reconstruct either with a reduced basis or everything.  It isn't necesarry but is interesting to show that we can still derive the correct PDE with the first 50 modes (maybe less). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f73FwrqWHZke"
      },
      "source": [
        "n,m,steps=Un.shape\n",
        "U = Un.reshape(n*m,steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmpXzve4HZk9"
      },
      "source": [
        "## Construct $\\Theta (U)$ and compute $U_t$\n",
        "\n",
        "Take derivatives and assemble into $\\Theta(\\omega, u ,v)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJwLfdmhxGIK",
        "outputId": "8435ea34-824f-4c39-c837-202deea89691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "X_data = u_f.reshape(len(u_f),1)\n",
        "\n",
        "X_ders = np.hstack([np.ones((len(u_f),1)), Ux_a.reshape(len(u_f),1), Uy_a.reshape(len(u_f),1)\n",
        "                    , Uxx_a.reshape(len(u_f),1), Uyy_a.reshape(len(u_f),1)])\n",
        "#X_ders = np.hstack([np.ones(uxx.shape), ux, uy, uxx, uxy])\n",
        "X_ders_descr = ['','u_{x}', 'u_{y}','u_{xx}','u_{yy}']\n",
        "X, description = build_Theta(X_data, X_ders, X_ders_descr, 1, data_description = ['u'])\n",
        "X_1=X[:,1:]\n",
        "print ('Candidate terms for PDE')\n",
        "description[1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Candidate terms for PDE\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['u_{x}',\n",
              " 'u_{y}',\n",
              " 'u_{xx}',\n",
              " 'u_{yy}',\n",
              " 'u',\n",
              " 'uu_{x}',\n",
              " 'uu_{y}',\n",
              " 'uu_{xx}',\n",
              " 'uu_{yy}']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0a2cRBxHfug"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GmIEE0oHZlK"
      },
      "source": [
        "## Solve for $\\xi$\n",
        "\n",
        "TrainSTRidge splits the data up into 80% for training and 20% for validation.  It searches over various tolerances in the STRidge algorithm and finds the one with the best performance on the validation set, including an $\\ell^0$ penalty for $\\xi$ in the loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI074I6JHZlM",
        "outputId": "e942317a-4613-4ab1-ddc6-4043d43ca8ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "lam = 10**-5\n",
        "d_tol = 5\n",
        "idx = np.random.choice(len(Ut_a),2000, replace=False)\n",
        "X_ = X_1[idx,:]\n",
        "print(X_.shape, X.shape)\n",
        "Ut_a_ = Ut_a[idx]\n",
        "c = TrainSTRidge(X_,Ut_a_.reshape(len(Ut_a_),1),lam,d_tol)\n",
        "print_pde(c, description[1:], ut = 'u_t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 9) (625000, 10)\n",
            "u_t = (-2.000000 +0.000000i)u_{x}\n",
            "    + (-0.500000 +0.000000i)u_{y}\n",
            "    + (1.500000 +0.000000i)u_{xx}\n",
            "    + (1.500000 +0.000000i)u_{yy}\n",
            "    + (2.000000 +0.000000i)u\n",
            "   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:405: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:550: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:576: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:580: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icTISNEYFO8R"
      },
      "source": [
        "$u_t = (1.137957 +0.000000i)\n",
        "    + (-0.237221 +0.000000i)u_{xx}\n",
        "    + (-0.292316 +0.000000i)u^2\n",
        "    + (1.261013 +0.000000i)u\n",
        "    + (-0.175449 +0.000000i)u^2u_{y}\n",
        "    + (0.661939 +0.000000i)uu_{y}\n",
        "    + (-0.107964 +0.000000i)u^2u_{yy}\n",
        "    + (0.352070 +0.000000i)uu_{yy}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCmQ7zkMHZlV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}